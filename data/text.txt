В этом уроке мы познакомимся с понятием кодировка и научимся применять его в своем коде, чтобы избегать недоразумений при работе с текстовыми файлами.
В общем случае кодировка - это однозначное соответствие между подмножеством целых чисел (как правило идущих подряд) и некоторым набором символов. Ключевым понятием здесь является понятие символа. Это может быть как буква, может соответствовать звуку речи, а также может быть представлен графическим знаком. Проще говоря - это мельчайшая неделимая частица информации.
Например латинское "A" и кириллическое "А" - это разные  символы, потому что они употребляются в разных контекстах и несут в себе разную информацию.
Определяющим для любой кодировки является количество охватываемых ею кодов и, соответственно, символов. Поскольку тексты в компьютере хранятся в виде последовательности байтов, большинство кодировок естественным образом распадаются на однобайтовые, или восьмибитные, способные закодировать не больше 256 символов, и двухбайтовые, или шестнадцатибитные, чья емкость может достигать 65636 знакомест.